{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the text from the document and break down into individual sentences\n",
    "doc = Document(\"test.docx\")\n",
    "\n",
    "text_list = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text_list.append(sent_tokenize(paragraph.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the document to lower case and break the sentences into words\n",
    "word_list = []\n",
    "tokenizer = RegexpTokenizer(\"['/-/$Â£a-zA-Z0-9]+\", gaps=False)\n",
    "for i in range(len(text_list)):\n",
    "    for text in text_list[i]:\n",
    "        word_list.append(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "from\n",
      "in\n",
      "of\n",
      "to\n",
      "by\n",
      "and\n",
      "by\n",
      "up\n",
      "to\n",
      "of\n",
      "will\n",
      "and\n",
      "to\n",
      "from\n",
      "it\n",
      "to\n",
      "the\n",
      "in\n",
      "in\n",
      "the\n",
      "of\n",
      "the\n",
      "the\n",
      "out\n",
      "to\n",
      "it\n",
      "by\n",
      "between\n",
      "and\n",
      "while\n",
      "it\n",
      "by\n",
      "to\n",
      "for\n",
      "the\n",
      "the\n",
      "will\n",
      "to\n",
      "the\n",
      "the\n",
      "in\n",
      "that\n",
      "the\n",
      "to\n",
      "of\n",
      "the\n",
      "of\n",
      "it\n",
      "out\n",
      "in\n",
      "but\n",
      "the\n",
      "be\n",
      "under\n",
      "from\n",
      "the\n",
      "have\n",
      "at\n",
      "their\n",
      "in\n",
      "a\n",
      "a\n",
      "by\n",
      "the\n",
      "of\n",
      "the\n",
      "which\n",
      "for\n",
      "and\n",
      "which\n",
      "will\n",
      "the\n",
      "s\n",
      "the\n",
      "the\n",
      "and\n",
      "of\n",
      "our\n",
      "the\n",
      "the\n",
      "of\n",
      "and\n",
      "be\n",
      "but\n",
      "in\n",
      "the\n",
      "he\n",
      "to\n",
      "it\n",
      "of\n",
      "to\n",
      "it\n",
      "to\n",
      "a\n",
      "but\n",
      "in\n",
      "the\n",
      "and\n",
      "on\n",
      "it\n",
      "had\n",
      "to\n",
      "up\n",
      "it\n",
      "to\n",
      "a\n",
      "of\n",
      "a\n",
      "between\n",
      "and\n",
      "a\n",
      "of\n",
      "it\n",
      "as\n",
      "under\n",
      "up\n",
      "to\n",
      "the\n",
      "or\n",
      "on\n",
      "at\n",
      "the\n",
      "of\n",
      "by\n",
      "more\n",
      "than\n",
      "when\n",
      "the\n",
      "of\n",
      "the\n",
      "a\n",
      "in\n",
      "from\n",
      "the\n",
      "s\n",
      "in\n",
      "the\n",
      "at\n",
      "of\n",
      "on\n",
      "from\n",
      "a\n",
      "in\n",
      "the\n",
      "of\n",
      "the\n",
      "an\n",
      "at\n",
      "if\n",
      "below\n",
      "a\n",
      "s\n",
      "a\n",
      "that\n",
      "s\n",
      "too\n",
      "to\n",
      "that\n",
      "s\n",
      "the\n",
      "for\n",
      "he\n",
      "the\n",
      "s\n",
      "to\n",
      "in\n",
      "the\n",
      "but\n",
      "the\n",
      "of\n",
      "the\n",
      "it\n",
      "is\n",
      "out\n",
      "of\n",
      "it\n",
      "there\n",
      "s\n",
      "no\n",
      "how\n",
      "the\n",
      "of\n",
      "and\n",
      "will\n"
     ]
    }
   ],
   "source": [
    "#Remove all stopwords and calculate word frequency\n",
    "frequency_table = dict()\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "for words in word_list:\n",
    "    for word in words:\n",
    "        word = pstem.stem(word)\n",
    "        if word in stoplist:\n",
    "            print(word)\n",
    "        elif word in frequency_table:\n",
    "            frequency_table[word] += 1\n",
    "        else:\n",
    "            frequency_table[word] = 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for scoring a sentence by its words\n",
    "\n",
    "sentence_weight = pd.DataFrame(columns=['sentence','weight'])\n",
    "index = 0\n",
    "\n",
    "for sentence in word_list: \n",
    "    weight = 0\n",
    "    for word in frequency_table:\n",
    "        if word in sentence:\n",
    "            weight += frequency_table[word]\n",
    "            \n",
    "    av_weight = weight/len(sentence)       \n",
    "    sentence_weight.loc[index] = [sentence,av_weight]\n",
    "    index += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = sentence_weight['weight'].mean()\n",
    "final = []\n",
    "\n",
    "for weight in sentence_weight['weight']:\n",
    "    if weight >= threshold:\n",
    "        final.append(sentence_weight['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             sentence    weight\n",
      "19  [that, s, the, rub, for, shell, investors, he,...  1.888889\n",
      "1   [anglo, dutch, oil, giant, to, cut, operating,...  1.823529\n",
      "4   [royal, dutch, shell, plans, to, slash, $9bn, ...  1.692308\n",
      "0   [shell, plans, to, slash, $9bn, from, spending...  1.545455\n",
      "2   [ben, van, beurden, chief, executive, of, roya...  1.529412\n",
      "6   [the, anglo, dutch, company, will, also, suspe...  1.523810\n",
      "10  [shell, boss, ben, van, beurden, said, the, ne...  1.518519\n",
      "11  [the, combination, of, steeply, falling, oil, ...  1.481481\n",
      "18  [nicholas, hyett, an, equity, analyst, at, har...  1.354839\n",
      "13  [shell, had, planned, to, spend, before, rampi...  1.272727\n",
      "7   [shell, hopes, that, decisive, action, could, ...  1.125000\n",
      "14  [oil, company, green, investments, although, a...  0.950000\n",
      "16  [the, market, value, of, shell, has, already, ...  0.909091\n",
      "20  [the, group, s, taking, emergency, action, to,...  0.866667\n",
      "8   [it, pays, out, $16bn, in, dividends, every, y...  0.857143\n",
      "5   [the, oil, giant, set, out, plans, to, reduce,...  0.838710\n",
      "9   [oil, prices, have, collapsed, at, their, fast...  0.800000\n",
      "12  [shell, would, continue, to, progress, its, pi...  0.800000\n",
      "3                [photograph, sergio, moraes/reuters]  0.666667\n",
      "17  [shares, in, the, ftse, 100, company, traded, ...  0.615385\n",
      "15  [sign, up, to, the, daily, business, today, em...  0.500000\n",
      "21  [there, s, no, knowing, exactly, how, long, th...  0.473684\n"
     ]
    }
   ],
   "source": [
    "print(sentence_weight.sort_values(by=['weight'],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordListCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6e256e216787>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 min_font_size = 5).generate(words)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# plot the WordCloud image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nengi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \"\"\"\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nengi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nengi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_word_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m             \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordListCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "words = ''\n",
    "\n",
    "for word in frequency_table:\n",
    "        words += \"\".join(str(word))+ \" \"\n",
    "        \n",
    "wordcloud = WordCloud(width = 1200, height = 1200, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 5).generate(words)\n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (20, 15), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('there', 's', 'no', 'knowing', 'exactly', 'how', 'long')]\n"
     ]
    }
   ],
   "source": [
    "#first try\n",
    "article_summary = []\n",
    "\n",
    "for sentence in sentence_weight:\n",
    "    if sentence_weight[sentence] >= threshold:\n",
    "        article_summary.append(sentence)\n",
    "\n",
    "print(article_summary)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
